{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for US confirmed cases and deaths can be found at the following links. Worldwide data is also available in the same repository.\n",
    "\n",
    "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\n",
    "\n",
    "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\n",
    "\n",
    "\n",
    "Download US sequential data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching time_series_covid19_confirmed_US.csv ...done\n",
      "Fetching time_series_covid19_deaths_US.csv ...done\n",
      "Fetching USA_Counties_with_FIPS_and_names.svg ...done\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# URLs\n",
    "confirmed_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "deaths_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "counties_map_url = 'https://upload.wikimedia.org/wikipedia/commons/5/5f/USA_Counties_with_FIPS_and_names.svg'\n",
    "\n",
    "\n",
    "# File names\n",
    "confirmed_file_name = confirmed_url.split('/')[-1]\n",
    "deaths_file_name = deaths_url.split('/')[-1]\n",
    "counties_map_file_name = counties_map_url.split('/')[-1]\n",
    "\n",
    "\n",
    "# Folders\n",
    "base_folder = os.getcwd()\n",
    "download_folder = base_folder + '/data/'\n",
    "map_folder = base_folder + '/map/'\n",
    "\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)\n",
    "    \n",
    "if not os.path.exists(map_folder):\n",
    "    os.makedirs(map_folder)\n",
    "\n",
    "\n",
    "# Communication \"The Art Of Communication Is The Language Of Leadership.\" Wow, I'm inspired.\n",
    "print('Fetching {} ...'.format(confirmed_file_name), end='')\n",
    "urllib.request.urlretrieve(confirmed_url, download_folder + confirmed_file_name)\n",
    "print('done')\n",
    "\n",
    "print('Fetching {} ...'.format(deaths_file_name), end='')\n",
    "urllib.request.urlretrieve(deaths_url, download_folder + deaths_file_name)\n",
    "print('done')\n",
    "\n",
    "print('Fetching {} ...'.format(counties_map_file_name), end='')\n",
    "urllib.request.urlretrieve(counties_map_url, map_folder + counties_map_file_name)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put the data from the csv into a numpy array\n",
    "\n",
    "\n",
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /Users/ethanblagg/SynologyDrive/Documents/Projects/YT/Covid/data/time_series_covid19_confirmed_US.csv\n",
      "\n",
      "uh yeah no. Nice try, data at (3147, 4)\n",
      "uh yeah no. Nice try, data at (3148, 4)\n",
      "[[   60     0     0 ...     0     0     0]\n",
      " [   66     0     0 ...   121   128   130]\n",
      " [   69     0     0 ...     6     6     6]\n",
      " ...\n",
      " [90055     0     0 ...     0     0     0]\n",
      " [90056     0     0 ...     0     0     0]\n",
      " [99999     0     0 ...   103   103   103]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file = download_folder + confirmed_file_name\n",
    "print(\"Opening \" + file + '\\n')\n",
    "\n",
    "# open the csv file\n",
    "with open(file, newline='') as csvfile:\n",
    "    covid_confirmed_us = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "    # iterate through the headers file, but save it\n",
    "    headers_confirmed = next(covid_confirmed_us)\n",
    "    row_length = len(headers_confirmed)\n",
    "    \n",
    "    # get headers before first day of data (11)\n",
    "    #for i, val in enumerate(headers):\n",
    "    #    print(\"{}: {}\".format(i,val))\n",
    "    first_day_idx = 11                                  # index of first day in row\n",
    "    FIPS_idx = 4                                        # FIPS code is a county code used to edit map\n",
    "    num_days = row_length-first_day_idx                 # number of days of data     \n",
    "    confirmed_data = np.empty((0,num_days+1), int)      # rows = counties,  cols = [id, day0, day1...]\n",
    "    \n",
    "    \n",
    "    # Wave the data to matrix. One county per row\n",
    "    for i, row in enumerate(covid_confirmed_us):\n",
    "        \n",
    "        temp_row = np.zeros((1, num_days+1), int)\n",
    "        \n",
    "        try:\n",
    "            temp_row[0][0] = int(float(row[FIPS_idx]))                # place county id in 0th index\n",
    "        except:\n",
    "            print(\"uh yeah no. Nice try, data at ({}, {})\".format(i, FIPS_idx))\n",
    "        \n",
    "        \n",
    "        temp_row[0][1:] = row[first_day_idx:]                         # copy days data to temp, after county id\n",
    "        confirmed_data = np.append(confirmed_data, temp_row, axis=0)  # add the temp_row as a new row\n",
    "        \n",
    "           \n",
    "    \n",
    "    print(confirmed_data)                                             # confirm everything looks a-ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /Users/ethanblagg/SynologyDrive/Documents/Projects/YT/Covid/data/time_series_covid19_confirmed_US.csv\n",
      "\n",
      "Can't find FIPS data for Dukes and Nantucket,Massachusetts,US\n",
      "Can't find FIPS data for Kansas City,Missouri,US\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   3   3   5  12  14  15  27  29  32  37  45  51  55  56  58  69  77  82\n",
      "  84  93 112 113 121 121 128 130]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  20  21  21  22  23  23  30  28  28  28  28  28 103 103 103 103 103 103\n",
      " 103 103 103 103 103 103 103 103]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file = download_folder + confirmed_file_name\n",
    "print(\"Opening \" + file + '\\n')\n",
    "\n",
    "# open the csv file\n",
    "with open(file, newline='') as csvfile:\n",
    "    covid_confirmed_us = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "    # iterate through the headers file, but save it\n",
    "    headers_confirmed = next(covid_confirmed_us)\n",
    "    row_length = len(headers_confirmed)\n",
    "    \n",
    "    # get headers before first day of data (11)\n",
    "    #for i, val in enumerate(headers):\n",
    "    #    print(\"{}: {}\".format(i,val))\n",
    "    first_day_idx = 11                                  # index of first day in row\n",
    "    FIPS_idx = 4                                        # FIPS code is a county code used to edit map\n",
    "    num_days = row_length-first_day_idx                 # number of days of data     \n",
    "    confirmed_data = np.zeros((100000,num_days), int) # rows = counties,  cols = [id, day0, day1...]\n",
    "    \n",
    "    \n",
    "    # Move the data to matrix. One county per row\n",
    "    for i, row in enumerate(covid_confirmed_us):\n",
    "        \n",
    "        #temp_row = np.zeros((1, num_days), int)\n",
    "        \n",
    "        try:\n",
    "            idx = int(float(row[FIPS_idx]))                # place county id in 0th index\n",
    "            #temp_row[0][:] = row[first_day_idx:]                         # copy days data to temp, after county id\n",
    "            confirmed_data[idx][:] = row[first_day_idx:]\n",
    "        except:\n",
    "            print(\"Can't find FIPS data for {}\".format(row[10]))\n",
    "        \n",
    "        \n",
    "           \n",
    "    \n",
    "    print(confirmed_data[66])                                             # confirm everything looks a-ok\n",
    "    print(confirmed_data[99999])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#out_file = 'Covid_confirmed_map_' + time.strftime(\"%y-%m-%d\") + \".svg\"\n",
    "out_file = 'We\\'re_all_gonna_die_' + time.strftime(\"%y-%m-%d\") + \".svg\"\n",
    "\n",
    "svg = open(map_folder + counties_map_file_name, 'r').read()    # Load the SVG map\n",
    "soup = BeautifulSoup(svg)                                      # Load into Beautiful Soup\n",
    "paths = soup.findAll('path')                                   # Find counties\n",
    "\n",
    "\n",
    "# display rules for each county. Note fill value will be added later, according to data\n",
    "path_style = 'font-size:12px;fill-rule:nonzero;stroke:#FFFFFF;stroke-opacity:1;stroke-width:0.1;stroke-miterlimit:4;stroke-dasharray:none;stroke-linecap:butt;marker-start:none;stroke-linejoin:bevel;fill:'\n",
    "\n",
    "\n",
    "for p in paths:\n",
    "     \n",
    "    if p['id'] not in [\"State_Lines\", \"separator\"]:\n",
    "        \n",
    "        try:\n",
    "            idx = int(p['id'].split(\"_\")[-1])\n",
    "            #print(idx)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        color = \"#d0d0d0\"                                              # default if no data\n",
    "        \n",
    "        colors = [\"#66ffff\",\"#66ff66\",\"#ffff66\",\"#ffb366\",\"#ff6666\"]   # alarming, yet calming\n",
    "        #colors = [\"#b3ffff\",\"#ccffcc\",\"#ffff99\",\"#ffcc99\",\"#ff9999\"]  # wussy\n",
    "        #colors = [\"#00FFFF\",\"#00FF00\",\"#FFFF00\",\"#FF7F00\",\"#FF0000\"]  # bold\n",
    "        \n",
    "        \n",
    "        if (confirmed_data[idx][num_days-1] < 1):\n",
    "            color = colors[0]\n",
    "            \n",
    "        elif (confirmed_data[idx][num_days-1] < 10):\n",
    "            color = colors[1]\n",
    "            \n",
    "        elif (confirmed_data[idx][num_days-1] < 100):\n",
    "            color = colors[2]\n",
    "            \n",
    "        elif (confirmed_data[idx][num_days-1] < 1000):\n",
    "            color = colors[3]\n",
    "            \n",
    "        else:\n",
    "            color = colors[4]\n",
    "                                     \n",
    "        p['style'] = path_style + color\n",
    "\n",
    "        \n",
    "#print(soup.prettify())\n",
    "\n",
    "\n",
    "f = open(map_folder + out_file, 'w')\n",
    "f.write(soup.prettify())\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
